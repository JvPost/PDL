{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions.normal as dist\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "trainset = datasets.MNIST(root='./data',download=False, train=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', download=False, transform=transform)\n",
    "\n",
    "# Prepare the training set\n",
    "train_features = torch.cat([data[0].view((1,28*28)) for data in trainset])\n",
    "train_labels = torch.Tensor([data[1] for data in trainset]).type(torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearVariational(nn.Module):    \n",
    "    def __init__(self, in_features, out_features, num_samples, prior_scale, mean_forward=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_samples = num_samples\n",
    "        self.mean_forward = mean_forward\n",
    "        \n",
    "        self.prior = dist.Normal(loc = torch.zeros(out_features, in_features),\n",
    "                                 scale=prior_scale*torch.ones(out_features, in_features))\n",
    "        \n",
    "        self.mu_W = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.s_W = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "\n",
    "    def get_weight_samples(self, num_samples):\n",
    "        # reparameterize\n",
    "        epsilon_dist = dist.Normal(loc = torch.zeros(self.mu_W.size()[0], self.mu_W.size()[1]),\n",
    "                                                    scale = torch.ones(self.mu_W.size()[0], self.mu_W.size()[1]))\n",
    "        epsilon = epsilon_dist.sample((num_samples,))\n",
    "        weights = self.mu_W + F.softplus(self.s_W)*epsilon\n",
    "        return weights\n",
    "    \n",
    "    def linear(self, weight_samples, x):\n",
    "        return torch.matmul(weight_samples.unsqueeze(0), x.unsqueeze(1).unsqueeze(3)).squeeze()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        weight_samples = self.get_weight_samples(self.num_samples)\n",
    "        out = self.linear(weight_samples, x)\n",
    "        if (self.mean_forward):\n",
    "            return out.mean(1)\n",
    "        else:\n",
    "            return out\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Variational_Network(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_samples):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latents = LinearVariational(in_features=50,\n",
    "                              out_features=10, \n",
    "                              num_samples=num_samples,\n",
    "                              prior_scale=5.,\n",
    "                              mean_forward=False)\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            # nn.Linear(28*28, 50),\n",
    "            LinearVariational(in_features=28*28,\n",
    "                              out_features=50,\n",
    "                              num_samples=num_samples,\n",
    "                              prior_scale=5.),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            self.latents\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    # def evaluate_log_prior(prior, weight_samples):\n",
    "    #     return torch.mean(prior.log_prob(weight_samples))\n",
    "\n",
    "    # def evaluate_log_q(mean, scale, weight_samples):\n",
    "    #     q_dist = dist.Normal(mean, F.softplus(scale))\n",
    "    #     return torch.mean(q_dist.log_prob(weight_samples))\n",
    "\n",
    "    \n",
    "    \n",
    "    def evaluate_log_likelihood(self, features, labels):\n",
    "        # preds = self.linear(weight_samples, features) # performs forward, which I want to do explicitly\n",
    "        preds = self.forward(features)\n",
    "        data_batch, param_batch, num_classes = preds.shape\n",
    "        repeated_labels = torch.cat([y.repeat((param_batch,)) for y in labels])\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        log_lk_loss = loss(preds.reshape((data_batch*param_batch, num_classes)), repeated_labels)\n",
    "        log_lk_loss = -log_lk_loss.reshape((data_batch, param_batch))\n",
    "        return torch.mean(torch.sum(log_lk_loss, 0))\n",
    "\n",
    "\n",
    "    def evaluate_ELBO(self, features, labels, correction = 1):\n",
    "        # weight_samples = self.get_weight_samples(num_samples)\n",
    "        avg_log_lk = self.evaluate_log_likelihood(features, labels) # y\n",
    "        q = torch.distributions.normal.Normal(self.latents.mu_W, F.softplus(self.latents.s_W))\n",
    "        return correction * (avg_log_lk + torch.mean(torch.distributions.kl.kl_divergence(self.latents.prior, q)))\n",
    "    \n",
    "    \n",
    "    def compute_marginalized_predictions(self, features, num_samples):\n",
    "        pre_preds = self.forward(features)\n",
    "        preds = nn.Softmax(dim=2)(pre_preds)\n",
    "        return torch.mean(preds, 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# no minibatching\n",
    "D = 25\n",
    "sub_train_features = train_features[:D,:]\n",
    "sub_train_labels = train_labels[:D]\n",
    "\n",
    "\n",
    "epochs = 2000\n",
    "num_samples = 25\n",
    "\n",
    "variational_network = Variational_Network(in_size=28*28, hidden_size=50, out_size=10, num_samples=num_samples)\n",
    "optimizer = optim.Adam(variational_network.parameters(), lr=0.001)\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = -variational_network.evaluate_ELBO(sub_train_features, sub_train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_list.append(float(loss.detach().numpy()))\n",
    "    \n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini batching\n",
    "batch_size = 50\n",
    "epochs = 50\n",
    "num_samples = 25\n",
    "variational_network = Variational_Network(in_size=28*28, hidden_size=50, out_size=10, num_samples=num_samples)\n",
    "optimizer = optim.Adam(variational_network.parameters(), lr=0.001)\n",
    "data_size = train_features.size()[0]\n",
    "correction_factor = data_size / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train mini w/ batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 took 72. seconds: -868802.3125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000010?line=8'>9</a>\u001b[0m sub_train_features \u001b[39m=\u001b[39m train_features[idxs]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000010?line=9'>10</a>\u001b[0m sub_train_labels \u001b[39m=\u001b[39m train_labels[idxs]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000010?line=11'>12</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mvariational_network\u001b[39m.\u001b[39;49mevaluate_ELBO(sub_train_features, sub_train_labels, correction\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000010?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000010?line=13'>14</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb Cell 5'\u001b[0m in \u001b[0;36mVariational_Network.evaluate_ELBO\u001b[0;34m(self, features, labels, correction)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=43'>44</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_ELBO\u001b[39m(\u001b[39mself\u001b[39m, features, labels, correction \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=44'>45</a>\u001b[0m     \u001b[39m# weight_samples = self.get_weight_samples(num_samples)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=45'>46</a>\u001b[0m     avg_log_lk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_log_likelihood(features, labels) \u001b[39m# y\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=46'>47</a>\u001b[0m     q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mnormal\u001b[39m.\u001b[39mNormal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatents\u001b[39m.\u001b[39mmu_W, F\u001b[39m.\u001b[39msoftplus(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatents\u001b[39m.\u001b[39ms_W))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m correction \u001b[39m*\u001b[39m (avg_log_lk \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mkl\u001b[39m.\u001b[39mkl_divergence(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatents\u001b[39m.\u001b[39mprior, q)))\n",
      "\u001b[1;32m/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb Cell 5'\u001b[0m in \u001b[0;36mVariational_Network.evaluate_log_likelihood\u001b[0;34m(self, features, labels)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_log_likelihood\u001b[39m(\u001b[39mself\u001b[39m, features, labels):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=33'>34</a>\u001b[0m     \u001b[39m# preds = self.linear(weight_samples, features) # performs forward, which I want to do explicitly\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=34'>35</a>\u001b[0m     preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=35'>36</a>\u001b[0m     data_batch, param_batch, num_classes \u001b[39m=\u001b[39m preds\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=36'>37</a>\u001b[0m     repeated_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([y\u001b[39m.\u001b[39mrepeat((param_batch,)) \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m labels])\n",
      "\u001b[1;32m/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb Cell 5'\u001b[0m in \u001b[0;36mVariational_Network.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000005?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=140'>141</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/container.py?line=141'>142</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/marco/miniconda3/envs/pdl/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb Cell 4'\u001b[0m in \u001b[0;36mLinearVariational.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=26'>27</a>\u001b[0m     weight_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_weight_samples(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=27'>28</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(weight_samples, x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean_forward):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=29'>30</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\u001b[39m.\u001b[39mmean(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb Cell 4'\u001b[0m in \u001b[0;36mLinearVariational.linear\u001b[0;34m(self, weight_samples, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=22'>23</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlinear\u001b[39m(\u001b[39mself\u001b[39m, weight_samples, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/marco/repos/PDL/Notebooks/variational_inference_assignment.ipynb#ch0000003?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(weight_samples\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m3\u001b[39;49m))\u001b[39m.\u001b[39msqueeze()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(data_size)\n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(0, data_size, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        idxs = permutation[i: i+batch_size]\n",
    "        sub_train_features = train_features[idxs]\n",
    "        sub_train_labels = train_labels[idxs]\n",
    "        \n",
    "        loss = -variational_network.evaluate_ELBO(sub_train_features, sub_train_labels, correction=1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_list.append(float(loss.detach().numpy()))   \n",
    "    end = time.time() \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"epoch {epoch} took {str(end - start)[:3]} seconds:\", float(loss.detach().numpy()))\n",
    "        \n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "test_N = 100\n",
    "val_N = 60000 - test_N\n",
    "test_set, val_set = torch.utils.data.random_split(testset, [test_N, val_N])\n",
    "\n",
    "\n",
    "# Prepare test set\n",
    "test_features = torch.cat([data[0].view((1,28*28)) for data in test_set])\n",
    "test_labels = torch.Tensor([data[1] for data in test_set]).type(torch.long)\n",
    "\n",
    "preds = variational_network.compute_marginalized_predictions(test_features, num_samples=100)\n",
    "\n",
    "print(\"accuracy:\", np.sum(np.array(torch.argmax(preds,1) == test_labels))/test_labels.shape[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c6d30d1bb4cb219ee12510515b98691a951bcdca6cc40a23427a1d9ab956ff0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('PDL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
