{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions as dist\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "use_gpu = True\n",
    "device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20.2%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /home/marco/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/marco/data/MNIST/raw/train-images-idx3-ubyte.gz to /home/marco/data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /home/marco/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/marco/data/MNIST/raw/train-labels-idx1-ubyte.gz to /home/marco/data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /home/marco/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/marco/data/MNIST/raw/t10k-images-idx3-ubyte.gz to /home/marco/data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /home/marco/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting /home/marco/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to /home/marco/data/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/anaconda3/envs/d2l/lib/python3.9/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /tmp/pip-req-build-28c20jpw/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "trainset = datasets.MNIST(root='~/data',download=True, train=True, transform=transform)\n",
    "testset = datasets.MNIST(root='~/data', download=True, transform=transform)\n",
    "\n",
    "# Prepare the training set\n",
    "train_features = torch.cat([data[0].view((1,28*28)) for data in trainset])\n",
    "train_labels = torch.Tensor([data[1] for data in trainset]).type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical you will learn how to implement supervised variational deep learning methods. The starting point is a multi-class logistic regression on the MNIST dataset.\n",
    "\n",
    "The forward model is just the linear equation\n",
    "$$\n",
    "f(x) = W x\n",
    "$$\n",
    "where $W$ is a (number of input pixels)x(number of classes) matrix.\n",
    "The forward model defines the class probabilities through a softmax transformation, which make the output positive and normalized:\n",
    "$$\n",
    "p(y_n = j \\mid x_n, W) = \\frac{e^{f_j(x)}}{\\sum_{k=1}^J e^{f_j(x)}}\n",
    "$$\n",
    "This results in the usual softmax cross-entropy negative log likelihood loss. Since we want to approximate the Bayesian posterior, we need to specify a prior over the weights. The simplest choice is a uncorrelated normal distribution:\n",
    "$$\n",
    "p(W) = \\prod_{j,k} p(W_{jk}) = \\prod_{j,k} \\mathcal{N}(W_{jk} \\mid 0, \\nu^2)~.\n",
    "$$\n",
    "In order to approximate the posterior, we also need to specify a parameterized variational distribution. The simplest choice is to use another parameterized uncorrelated normal distribution:\n",
    "$$\n",
    "q(W; M, S) = \\prod_{j,k} q(W_{jk}; M_{jk}, S_{jk}) = \\prod_{j,k}\\mathcal{N}(W_{jk} \\mid M_{jk}, S_{jk}^2)~.\n",
    "$$\n",
    "where $M$ and $S$ are arrays of learnable variational parameters. Note that we are assuming that the parameters are uncorrelated under the posterior distribution. This approximation is known as mean-field for historical reasons due to its origin in the analysis of the physics of magnets. As we saw in the lecture, we can train these parameters by minimizing the negative ELBO:\n",
    "$$\n",
    "\\mathcal{L}(\\mu^{(q)}, \\sigma^{(q)}) = - \\mathbb{E}_{W \\sim q(W; M, S)}\\left[\\log{\\frac{\\left(\\prod_{n=1}^N p(y_n \\mid x_n, W) \\right) \\prod_{j,k} \\mathcal{N}(W_{jk} \\mid 0, \\nu^2)}{\\prod_{j,k}\\mathcal{N}(W_{jk} \\mid M_{jk}, S_{jk}^2)}}\\right]\n",
    "$$\n",
    "Since we cannot evaluate the loss exactly, we use an unbiased gradient estimator using the reparameterization trick:\n",
    "$$\n",
    "\\mathcal{L}(\\mu^{(q)}, \\sigma^{(q)}) \\approx -\\frac{1}{L} \\sum_{m=1}^L \\left(\\log{\\frac{\\left(\\prod_{n=1}^N p(y_n \\mid x_n, M_{jk} + S_{jk} \\epsilon^{(m)}_{jk}) \\right) \\prod_{j,k} \\mathcal{N}(M_{jk} + S_{jk} \\epsilon^{(m)}_{jk} \\mid 0, \\nu^2)}{\\prod_{j,k}\\mathcal{N}(M_{jk} + S_{jk} \\epsilon^{(m)}_{jk} \\mid M_{jk}, S_{jk}^2)}}\\right)\n",
    "$$\n",
    "where $E_m$ is a matrix of random numbers sampled from uncorrelated standard normal distributions, a random number for each of the weights (remember, we have a posterior distribution for each entry of the forward model weight matrix).\n",
    "\n",
    "We are now ready to implement this in pytorch! If you are already familiar with Pytorch, most of the following code will be familiar to you. The only difficulty is that we need to perform some re-shaping gymnastic in order to perform batched operations where both the weights and the input have a batch dimension.\n",
    "This is required since a batch of weights need to be sampled from the variational posterior at every step in order to compute the Monte Carlo gradient estimator.\n",
    "\n",
    "We start by defining the variational logistic regression class. Be sure toi study the code (including the comments) in details!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 79400\n"
     ]
    }
   ],
   "source": [
    "class LinearVariational(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_samples, prior_scale):\n",
    "        super(LinearVariational, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # prior weights\n",
    "        self.prior = dist.normal.Normal(loc = torch.zeros(out_features, in_features),\n",
    "                                 scale = prior_scale*torch.ones(out_features, in_features))\n",
    "        \n",
    "        self.mu_W = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.s_W = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "    \n",
    "    def forward(self, x, num_samples):\n",
    "        sampled = self.sample(num_samples)\n",
    "        out = torch.matmul(sampled.unsqueeze(0), x.unsqueeze(1).unsqueeze(3)).squeeze()\n",
    "        return out\n",
    "        \n",
    "    def sample(self, num_samples):\n",
    "        epsilon_dist = dist.normal.Normal(loc = torch.zeros_like(self.mu_W),\n",
    "                                    scale = torch.ones_like(self.s_W))\n",
    "        e = epsilon_dist.sample((num_samples,))\n",
    "        weights = self.mu_W + F.softplus(self.s_W)*e\n",
    "        return weights\n",
    "    \n",
    "class VariationalNetwork(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_samples):\n",
    "        super(VariationalNetwork, self).__init__()\n",
    "        \n",
    "        self.var_1 = LinearVariational(in_features=28*28,\n",
    "                                      out_features=50,\n",
    "                                      num_samples=num_samples,\n",
    "                                      prior_scale=5.)\n",
    "        \n",
    "        self.var_2 = LinearVariational(in_features=50,\n",
    "                                    out_features=10,\n",
    "                                    num_samples=num_samples,\n",
    "                                    prior_scale=5.)\n",
    "        self.out_size = out_size\n",
    "        self.num_samples = num_samples\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.var_1(x, self.num_samples).mean(1)\n",
    "        x = self.var_2(x, self.num_samples)\n",
    "        return x\n",
    "    \n",
    "    def evaluate_log_likelihood(self, features, labels):\n",
    "        preds = self.forward(features)\n",
    "        data_batch, param_batch, num_class = preds.shape\n",
    "        repeated_labels = torch.cat([y.repeat((param_batch,)) for y in labels])\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "        log_lk_loss = loss(preds.reshape((data_batch*param_batch, self.out_size)), repeated_labels)\n",
    "        log_lk_loss = -log_lk_loss.reshape((data_batch, param_batch))\n",
    "        return torch.mean(torch.sum(log_lk_loss, 0))\n",
    "    \n",
    "    def evaluate_ELBO(self, features, labels, correction = 1):\n",
    "        avg_log_lk = self.evaluate_log_likelihood(features, labels)\n",
    "        q_1 = dist.normal.Normal(self.var_1.mu_W, F.softplus(self.var_1.s_W))\n",
    "        q_2 = dist.normal.Normal(self.var_2.mu_W, F.softplus(self.var_2.s_W))\n",
    "        log_prior = self.var_1.prior.log_prob(self.var_1.mu_W) + self.var_2.prior.log_prob(self.var_2.mu_W)\n",
    "        log_q_L = q_1.log_prob(self.var_1.mu_W) + q_2.log_prob(self.var_2.mu_W)\n",
    "        # return correction * avg_log_lk + torch.mean(dist.kl.kl_divergence(prior, q_L))\n",
    "        return correction * avg_log_lk + (log_prior - log_q_L)\n",
    "\n",
    "    def compute_marginalized_predictions(self, features, num_samples):\n",
    "        pre_preds = self.forward(features)\n",
    "        preds = nn.Softmax(dim=2)(pre_preds)\n",
    "        return torch.mean(preds, 1)\n",
    "\n",
    "        \n",
    "        \n",
    "# no minibatching\n",
    "D = 25\n",
    "sub_train_features = train_features[:D,:]\n",
    "sub_train_labels = train_labels[:D]\n",
    "\n",
    "\n",
    "epochs = 2000\n",
    "num_samples = 25\n",
    "\n",
    "vn = VariationalNetwork(in_size=28*28, hidden_size=50, out_size=10, num_samples=num_samples)\n",
    "optimizer = optim.Adam(vn.parameters(), lr=0.001)\n",
    "\n",
    "num_params = sum(p.numel() for p in vn.parameters())\n",
    "print(f\"Params: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (784) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4565/3436417020.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_ELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4565/2957700676.py\u001b[0m in \u001b[0;36mevaluate_ELBO\u001b[0;34m(self, features, labels, correction)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mq_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mq_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mlog_q_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# return correction * avg_log_lk + torch.mean(dist.kl.kl_divergence(prior, q_L))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (784) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = -vn.evaluate_ELBO(sub_train_features, sub_train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    loss_list.append(float(loss.detach().numpy()))\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(f\"epoch {epoch}, {loss.detach().numpy()}\")\n",
    "    \n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Normal' and 'Normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4565/1975560363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msub_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_ELBO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_train_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorrection_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4565/1550531107.py\u001b[0m in \u001b[0;36mevaluate_ELBO\u001b[0;34m(self, features, labels, correction)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mq_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mq_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mq_L\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorrection\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mavg_log_lk\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Normal' and 'Normal'"
     ]
    }
   ],
   "source": [
    "# mini batching\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "num_samples = 25\n",
    "vn = VariationalNetwork(in_size=28*28, hidden_size=50, out_size=10, num_samples=num_samples)\n",
    "vn.to(device)\n",
    "optimizer = optim.Adam(vn.parameters(), lr=0.001)\n",
    "data_size = train_features.size()[0]\n",
    "correction_factor = data_size / batch_size\n",
    "print(correction_factor)\n",
    "\n",
    "loss_list = []\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(data_size)\n",
    "    start = time.time()\n",
    "    \n",
    "    for i in range(0, data_size, batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        idxs = permutation[i: i+batch_size]\n",
    "        sub_train_features = train_features[idxs].to(device)\n",
    "        sub_train_labels = train_labels[idxs].to(device)\n",
    "        \n",
    "        loss = -vn.evaluate_ELBO(sub_train_features, sub_train_labels, correction=correction_factor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_list.append(float(loss.detach().numpy()))   \n",
    "    end = time.time() \n",
    "    \n",
    "    print(f\"epoch {epoch}/{epochs} took {str(end - start)[:4]} seconds:\", float(loss.detach().numpy()))\n",
    "        \n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9005875dd18c920bfab1bb2c8479d0e1e7ddd2401cd9723df4ee61e0d0148aca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('d2l')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
